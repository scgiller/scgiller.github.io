---
layout: post
title: Project 6
---

IMDb Ratings

Determining Factors that Influence Movie Ratings

In effort to predict movie popularity for Netflix according to IMDb ratings, I decided
to measure the Top 250 movies of all time against the Worst 100 movies of all time.
I wanted to know: is there something obvious that differentiates the best from the
worst?

Using APIs, JSON, and web scraping techniques, I pulled information regarding the
Best 250 movies and Worst 100 movies according to user votes on IMDb.com. The data accumulation process was largely successful: I was able to retrieve information for
249 of the Top 250 movies, and 85 of the Worst 100 movies. There were issues with
some foreign films as well as films with popular titles (more than one movie had
the same title).

Then, in an effort to predict whether a movie was more likely to be in the Top 250 or Bottom 100, I measured the number of IMDb votes, the length of the movie, the number of
languages spoken in the movie, the year the movie was released, the number of genres
that IMDb associates with that movie, whether the movie was rated R or not, and
whether the movie was released during Oscar season against a binary target variable
for Top 250.

Taking an initial look at some of the data:

Best and Worst Movie Distribution, by Rating

![movie_rating](/images/movie_rating_histogram.png)

As acknowledged above, most of our data comes from the Top 250 list.

Movie Rating by Year

![year_rating](/images/year_vs_imdbrating.png)

Notably, the movies rated as the worst ever were not released until around 1960.
There is also a noticeable dip in the last few years.

Movie Length by Year

![year_length](/images/year_vs_length.png)

There is not much of a trend over time in terms of movie length.

IMDb Votes by Year

![year_length](/images/year_vs_imdbvotes.png)

There is a clear positive relationship between year and IMDb votes. More recent
movies are voted on at a much higher rate.

IMDb Votes vs. IMDb Rating

![year_length](/images/imdbvotes_vs_imdbrating.png)

There is a clear difference between the amount of votes a movie received and the
score the movie receives. While some top rated movies do not receive many votes,
no poorly rated movies receive many votes.

I used four different models in an attempt to classify each movie: decision trees,
decision trees with bagging, extra trees, and random forests. Both extra trees and
random forests produced the best models by cross val score (.95-.98). It turned
out to be fairly straightforward to classify movies as best or worst. As expected
based on initial data visualization, IMDb votes had the highest feature importance
score, far outweighing the next three most important features (length, language count,
and year). Since we know the poorly rated movies do not receive many votes, I tried
running the models again without the IMDb votes feature.

The models were not as successful by cross val score (.85-.89), but I have more
confidence that the model is not subjected to any type of bias, specifically
multicollinearity. As measured by feature importance, length became the most important
feature. Looking back at the raw data, movies in the Top 250 averaged 129 minutes,
while movies in the Bottom 100 averaged 92 minutes.

Other possible features in future analysis would include: movie budget, box office
success, time the movie spent in theaters (or a flag if the movie did not even make
it to theaters), and a flag for "summer blockbuster" to mimic the "oscar bait" flag
that is currently included.

Once we feel comfortable that we can predict whether a movie will be terrible, I'd
like to present Netflix with two options:

1. These movies can be buried by all recommendation algorithms, so that future users
will never even know these movies exist.
2. Create a "Horrible Movies" category for people who like to watch these things
ironically.
